{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10151552,"sourceType":"datasetVersion","datasetId":6267162}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport xgboost as xgb\nimport optuna\n\n# Load the data\nX_train_1 = np.load('/kaggle/input/lfd-kaggle/train_feats.npy', allow_pickle=True).item()\nlabels_df = pd.read_csv('/kaggle/input/lfd-kaggle/train_labels.csv')\ny_train = labels_df['label'].values\n\nX_train_resnet = X_train_1['resnet_feature']\nX_train_vit = X_train_1['vit_feature']\nX_train_clip = X_train_1['clip_feature']\nX_train_dino = X_train_1['dino_feature']\nX_train_combined = np.concatenate([X_train_resnet, X_train_vit, X_train_clip, X_train_dino], axis=1)\n\n# Encode labels\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\n\n# Define the MLP model\nclass MLPFeatureExtractor(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(MLPFeatureExtractor, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n       \n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# Train the MLP model\ndef train_mlp(X_train, y_train, input_size, hidden_size, output_size, epochs=10, batch_size=64, learning_rate=0.01):\n    # Convert data to PyTorch tensors\n    X_tensor = torch.tensor(X_train, dtype=torch.float32)\n    y_tensor = torch.tensor(y_train, dtype=torch.long)\n\n    # Create a PyTorch dataset and DataLoader\n    dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    # Initialize the model, loss, and optimizer\n    model = MLPFeatureExtractor(input_size, hidden_size, output_size)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        for X_batch, y_batch in data_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n\n    # Extract features\n    model.eval()\n    with torch.no_grad():\n        X_transformed = model.fc1(X_tensor).numpy()  # Extract hidden-layer features\n\n    return X_transformed, model\n\n# Define hyperparameters for MLP\ninput_size = X_train_combined.shape[1]\nhidden_size = 16\noutput_size = len(np.unique(y_train_encoded))\n\n# Train the MLP and extract features\nX_transformed, mlp_model = train_mlp(X_train_combined, y_train_encoded, input_size, hidden_size, output_size)\n\n# Use transformed features for XGBoost\ndef objective_xgboost(trial):\n    params = {\n        'objective': 'multi:softmax',\n        'num_class': len(np.unique(y_train_encoded)),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'eta': trial.suggest_loguniform('eta', 0.01, 0.3),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'gamma': trial.suggest_float('gamma', 0, 10),\n        'lambda': trial.suggest_float('lambda', 0, 10),\n        'alpha': trial.suggest_float('alpha', 0, 10),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n        'seed': 42,\n        'tree_method': 'hist',  # Use GPU if available\n        'device':'cuda'\n    }\n\n    # Cross-validation\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    f1_scores = []\n\n    for train_idx, val_idx in skf.split(X_transformed, y_train_encoded):\n        X_train, X_val = X_transformed[train_idx], X_transformed[val_idx]\n        y_train, y_val = y_train_encoded[train_idx], y_train_encoded[val_idx]\n\n        dtrain = xgb.DMatrix(X_train, label=y_train)\n        dval = xgb.DMatrix(X_val, label=y_val)\n\n        model = xgb.train(params, dtrain, num_boost_round=trial.suggest_int('num_boost_round', 100, 500))\n\n        y_val_pred = model.predict(dval)\n        y_val_pred = np.round(y_val_pred).astype(int)\n\n        f1 = f1_score(y_val, y_val_pred, average='macro')\n        f1_scores.append(f1)\n\n    return np.mean(f1_scores)\n\n# Run Optuna for XGBoost\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective_xgboost, n_trials=10)\n\nprint(\"Best Parameters for XGBoost with MLP features:\", study.best_params)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:06:57.545280Z","iopub.execute_input":"2024-12-18T15:06:57.546059Z","iopub.status.idle":"2024-12-18T15:09:19.397773Z","shell.execute_reply.started":"2024-12-18T15:06:57.546025Z","shell.execute_reply":"2024-12-18T15:09:19.396841Z"}},"outputs":[{"name":"stderr","text":"[I 2024-12-18 15:07:09,782] A new study created in memory with name: no-name-d0c48903-98f9-44ef-b592-71a7fb28cd9c\n/tmp/ipykernel_23/3639997840.py:88: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.01, 0.3),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:09] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:12] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:14] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:17] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:19] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-12-18 15:07:22,514] Trial 0 finished with value: 0.9962259511037928 and parameters: {'max_depth': 9, 'eta': 0.03510567522547648, 'subsample': 0.8169612786091129, 'colsample_bytree': 0.8069291130176552, 'gamma': 4.4820917871314645, 'lambda': 6.758300052391572, 'alpha': 7.110351882050281, 'n_estimators': 442, 'num_boost_round': 267}. Best is trial 0 with value: 0.9962259511037928.\n/tmp/ipykernel_23/3639997840.py:88: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.01, 0.3),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:22] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:25] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:27] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:30] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:33] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-12-18 15:07:35,737] Trial 1 finished with value: 0.9962760004070248 and parameters: {'max_depth': 4, 'eta': 0.0802567270743116, 'subsample': 0.5879931167445216, 'colsample_bytree': 0.8373021840774447, 'gamma': 3.150304789281446, 'lambda': 8.142370366504458, 'alpha': 2.738733472635053, 'n_estimators': 223, 'num_boost_round': 392}. Best is trial 1 with value: 0.9962760004070248.\n/tmp/ipykernel_23/3639997840.py:88: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.01, 0.3),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:35] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:38] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:41] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:43] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:46] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-12-18 15:07:49,105] Trial 2 finished with value: 0.9962755189101579 and parameters: {'max_depth': 6, 'eta': 0.16914093182953183, 'subsample': 0.9330175753125176, 'colsample_bytree': 0.752363091073025, 'gamma': 5.423194603714494, 'lambda': 5.449410227389503, 'alpha': 2.7986949863325385, 'n_estimators': 205, 'num_boost_round': 425}. Best is trial 1 with value: 0.9962760004070248.\n/tmp/ipykernel_23/3639997840.py:88: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.01, 0.3),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:49] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:50] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:51] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:53] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:54] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-12-18 15:07:55,589] Trial 3 finished with value: 0.995851239741181 and parameters: {'max_depth': 8, 'eta': 0.03843038094386093, 'subsample': 0.5347125017706378, 'colsample_bytree': 0.91972937613138, 'gamma': 0.6049094890782425, 'lambda': 2.7619859275477676, 'alpha': 9.695287859460022, 'n_estimators': 135, 'num_boost_round': 102}. Best is trial 1 with value: 0.9962760004070248.\n/tmp/ipykernel_23/3639997840.py:88: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.01, 0.3),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:55] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:07:58] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:02] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:05] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:08] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-12-18 15:08:12,142] Trial 4 finished with value: 0.9960251743882571 and parameters: {'max_depth': 10, 'eta': 0.06428187346698681, 'subsample': 0.6442415317913702, 'colsample_bytree': 0.5460844385536412, 'gamma': 5.900472714301746, 'lambda': 8.984276741339823, 'alpha': 4.166107427090006, 'n_estimators': 493, 'num_boost_round': 466}. Best is trial 1 with value: 0.9962760004070248.\n/tmp/ipykernel_23/3639997840.py:88: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.01, 0.3),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:12] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:14] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:17] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:19] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:22] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-12-18 15:08:24,818] Trial 5 finished with value: 0.9959249224582407 and parameters: {'max_depth': 7, 'eta': 0.14848348182299828, 'subsample': 0.6191596964691525, 'colsample_bytree': 0.5201605773680407, 'gamma': 9.802079486178371, 'lambda': 6.345569726548211, 'alpha': 0.09447869945059306, 'n_estimators': 237, 'num_boost_round': 403}. Best is trial 1 with value: 0.9962760004070248.\n/tmp/ipykernel_23/3639997840.py:88: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.01, 0.3),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:24] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:26] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:28] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:30] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:32] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-12-18 15:08:34,134] Trial 6 finished with value: 0.9953758761988014 and parameters: {'max_depth': 7, 'eta': 0.020074292723496177, 'subsample': 0.721032472542819, 'colsample_bytree': 0.9017043072079853, 'gamma': 6.147019507896495, 'lambda': 7.937704777421066, 'alpha': 8.112471862293496, 'n_estimators': 350, 'num_boost_round': 172}. Best is trial 1 with value: 0.9962760004070248.\n/tmp/ipykernel_23/3639997840.py:88: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.01, 0.3),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:34] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:37] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:41] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:44] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:48] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-12-18 15:08:52,229] Trial 7 finished with value: 0.9959752863589362 and parameters: {'max_depth': 5, 'eta': 0.03000728996917385, 'subsample': 0.5820845324558637, 'colsample_bytree': 0.5266281057250686, 'gamma': 9.641292225892057, 'lambda': 8.623427607767159, 'alpha': 0.9270624041138087, 'n_estimators': 343, 'num_boost_round': 459}. Best is trial 1 with value: 0.9962760004070248.\n/tmp/ipykernel_23/3639997840.py:88: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.01, 0.3),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:52] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:55] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:08:58] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:09:01] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:09:04] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-12-18 15:09:08,124] Trial 8 finished with value: 0.9960251401743401 and parameters: {'max_depth': 3, 'eta': 0.019878443866871355, 'subsample': 0.553105608479847, 'colsample_bytree': 0.6589322619770743, 'gamma': 3.0936690916263423, 'lambda': 1.0199259035516328, 'alpha': 5.663673886235024, 'n_estimators': 409, 'num_boost_round': 340}. Best is trial 1 with value: 0.9962760004070248.\n/tmp/ipykernel_23/3639997840.py:88: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'eta': trial.suggest_loguniform('eta', 0.01, 0.3),\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:09:08] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:09:10] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:09:12] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:09:14] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:09:17] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"n_estimators\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n[I 2024-12-18 15:09:19,393] Trial 9 finished with value: 0.9952495975085908 and parameters: {'max_depth': 4, 'eta': 0.01646167988245542, 'subsample': 0.9620754512050953, 'colsample_bytree': 0.8034012158047388, 'gamma': 3.8398000505122742, 'lambda': 6.053526925164997, 'alpha': 2.3252166719519907, 'n_estimators': 448, 'num_boost_round': 201}. Best is trial 1 with value: 0.9962760004070248.\n","output_type":"stream"},{"name":"stdout","text":"Best Parameters for XGBoost with MLP features: {'max_depth': 4, 'eta': 0.0802567270743116, 'subsample': 0.5879931167445216, 'colsample_bytree': 0.8373021840774447, 'gamma': 3.150304789281446, 'lambda': 8.142370366504458, 'alpha': 2.738733472635053, 'n_estimators': 223, 'num_boost_round': 392}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\n#best_params = {'max_depth': 8, 'eta': 0.027112223293722308, 'subsample': 0.911504725837794, 'colsample_bytree': 0.9632668796201045, 'gamma': 6.656675330872401, 'lambda': 7.6387053729149805, 'alpha': 5.958534201965637, 'n_estimators': 453, 'num_round': 107}\nbest_params = study.best_params\n\nfinal_model = XGBClassifier(\n    **best_params,\n    use_label_encoder=False,\n    eval_metric=\"logloss\",\n    random_state=42,\n    tree_method=\"hist\",\n    device=\"cuda\"\n)\nfinal_model.fit(X_transformed, y_train_encoded)\n\n# Test Verisi ile Tahmin\nX_valtest = np.load('/kaggle/input/lfd-kaggle/valtest_feats.npy', allow_pickle=True).item()\nX_val_resnet = X_valtest['resnet_feature']\nX_val_vit = X_valtest['vit_feature']\nX_val_clip = X_valtest['clip_feature']\nX_val_dino = X_valtest['dino_feature']\n\nX_val_combined = np.concatenate([X_val_resnet, X_val_vit, X_val_clip, X_val_dino], axis=1)\n\n# Convert test data to PyTorch tensors\nX_val_tensor = torch.tensor(X_val_combined, dtype=torch.float32)\n# Use the trained MLP to extract features\nmlp_model.eval()  # Set the MLP model to evaluation mode\nwith torch.no_grad():\n    X_val_transformed = mlp_model.fc1(X_val_tensor).numpy()  # Extract hidden-layer features\n\n#dval = xgb.DMatrix(X_val_transformed)\n\n\npredictions = final_model.predict(X_val_transformed)\ny_val_pred_decoded = label_encoder.inverse_transform(predictions.astype(int))\n\n\n# Tahminleri Kaydetme\nprediction_df = pd.DataFrame({\n    'ID': np.arange(len(y_val_pred_decoded)),\n    'Predicted': y_val_pred_decoded\n})\nprediction_df.to_csv('xg_final.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T15:13:15.794614Z","iopub.execute_input":"2024-12-18T15:13:15.794979Z","iopub.status.idle":"2024-12-18T15:13:18.124448Z","shell.execute_reply.started":"2024-12-18T15:13:15.794950Z","shell.execute_reply":"2024-12-18T15:13:18.123273Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:13:15] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"num_boost_round\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"}],"execution_count":6}]}